import { MaskParam, Coord3D, FacePose, BodyMask, PosePoint, PoseDetection, HandPoint, PhalanxDetection, WristDetection, HandDetection } from '@geenee/bodytracking';
export { HandPoint, MeshDetection, PosePoint, WristDetection, WristLine } from '@geenee/bodytracking';
import { ProcParams, Processor, Size, ImageInput, Engine, EngineParams, VideoSource } from '@geenee/armature';

/** Parameters of {@link FaceProcessor} */
interface FaceParams extends ProcParams {
    /**
     * Evaluate transformation aligning reference face 3D model
     * with the measured one. Applying this transformation one can
     * align 3D object with the current pose (translation+rotation)
     * of the head. If the model's initial position is aligned with
     * the reference face, relative transformation will be preserved
     */
    transform?: boolean;
    /**
     * Evaluate metric 3D points - points within 3D space
     * of perspective camera located at the space origin
     * and pointed in the negative direction of the Z-axis.
     * These points can be used to apply texture face mask.
     */
    metric?: boolean;
    backproj?: boolean;
    /**
     * Evaluate with higher precision for lips, eyes and irises.
     * When enabled computation is slower, but contours of lips
     * are much more accurate and eye/iris detection is enabled.
     */
    highP?: boolean;
    /**
     * Evaluate body segmentation mask. Segmentation mask is a
     * monochrome image where every pixel has value from 0 to 1
     * denoting the probability of it being a foreground pixel.
     * Default resolution of a segmentation mask is 128x128.
     */
    mask?: MaskParam;
    /**
     * Extent of a segmentation mask box around detected face box.
     * This fraction of width/height will be added to the face rect
     * on all sides and mask will be evaluated in the extented rect.
     * If undefined segmentation will be done for the whole image.
     */
    maskExt?: number;
    /**
     * Size of a segmentation mask. Available sizes are 256, 192, 128;
     * 256x256 is the default. If `maskExt` is defined a square mask of
     * `maskSize` size will be evaluated, otherwise mask size will be
     * adjusted according to aspect ratio preserving the longer side.
     */
    maskSize?: 256 | 192 | 128;
}
/** Face detection */
interface Face {
    /**
     * 2D pixel face landmarks - points in the screen coordinate space.
     * X and Y coordinates are normalized screen coordinates (scaled by
     * width and height of the input image), while the Z coordinate is
     * depth within orthographic projection space. These points can be
     * used for 2D face filters or when using orthographic projection.
     */
    pixel: Coord3D[];
    /**
     * 3D metric points - points within 3D space of perspective camera
     * located at the space origin and pointed in the negative direction
     * of the Z-axis. These points can be used to apply texture face mask.
     */
    metric?: Coord3D[];
    backproj?: Coord3D[];
    /**
     * Face pose - transformation matrix (translation+rotation+scale)
     * aligning reference face 3D model with the measured 3D face mesh.
     * Applying this transformation one can align 3D object with the
     * detected face. If the model's initial position is aligned with
     * the reference face, relative transformation will be preserved.
     */
    transform?: FacePose;
    /**
     * Segmentation mask - monochrome image, where every pixel has value
     * from 0 to 1 denoting the probability of it being a foreground pixel.
     * Mask is provided for normalized rect region of the original image,
     * it has a fixed size in pixels and should be scaled to image space.
     */
    mask?: BodyMask;
    /** Reliability score, number between 0 and 1 */
    score: number;
}
/** Results of {@link FaceProcessor} */
interface FaceResult {
    faces: Face[];
}
/**
 * Face mesh processor
 *
 * Face mesh processor estimates 3D face landmarks, it detects and
 * tracks face mesh providing smooth, stable and accurate results.
 * Processor evaluates 2D pixel and 3D metric points as well as face
 * pose (translation+rotation+scale) aligning reference face model.
 * 2D pixel face landmarks - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space. These points can be
 * used for 2D face filters or when using orthographic projection.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used to apply texture face mask.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 * Face pose - transformation matrix (translation+rotation+scale)
 * aligning reference face 3D model with the measured 3D face mesh.
 * Applying this transformation one can align 3D object with the
 * detected face. If the model's initial position is aligned with
 * the reference face, relative transformation will be preserved.
 * Face processor may estimate an accurate & stable segmentation mask.
 * Segmentation mask - monochrome image, where every pixel has value
 * in range [0..1] denoting the probability of it being a foreground.
 * Mask is provided for normalized rect region of the original image,
 * it has a fixed size in pixels and should be scaled to image space.
 * Optional temporal smoothing of a segmentation mask may be enabled.
 * Estimated mask may be used for background substitution, effects like
 * bokeh or focal blur, advanced occluder materials utilizing a mask,
 * regional patchers, and other foreground/background shader effects.
 */
declare class FaceProcessor extends Processor<FaceResult, FaceParams> {
    /** Face mesh tracker (computation engine) */
    private faceTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /**
     * Initialize processor
     *
     * Prepares all resources required for face mesh tracking.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: FaceParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Face mesh processor detects and tracks faces.
     *
     * @param input - Image
     * @param timestamp - Image timestamp
     * @returns Face meshes
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<FaceResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Face mesh processor fixes FoV for more accurate estimation.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Face engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link FaceProcessor}. Straightforward wrapper,
 * instead of `new Engine(FaceProcessor, ...)` you can
 * use simpler `const engine = new FaceEngine(...})`.
 */
declare class FaceEngine extends Engine<FaceResult, FaceParams, FaceProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

/** Parameters of {@link PoseProcessor} */
interface PoseParams extends ProcParams {
    /**
     * Evaluate body segmentation mask
     *
     * Segmentation mask is a monochrome image having the same
     * size as input, where every pixel has value from 0 to 1
     * denoting the probability of it being a foreground pixel.
     */
    mask?: MaskParam;
}
/** Pose detection */
interface Pose {
    /**
     * List of pose {@link PosePoint | keypoints}
     *
     * 2D pixel coordinate - point in the screen coordinate space.
     * XY coordinates are normalized screen coordinates (scaled by
     * image width and height), while the Z coordinate is depth
     * in orthographic projection space, it has the same scale as X.
     * 3D metric coordinate - point within 3D space of perspective
     * camera located at the space origin and pointed in the negative
     * direction of the Z-axis. 3D & 2D points are perfectly aligned.
     */
    points: PosePoints;
    /**
     * Body segmentation mask
     *
     * Segmentation mask - monochrome image, where every pixel has value
     * from 0 to 1 denoting the probability of it being a foreground pixel.
     * Mask is provided for normalized rect region of the original image,
     * it has a fixed size in pixels and should be scaled to image space.
     */
    mask?: BodyMask;
    /** Reliability score, number between 0 and 1 */
    score: number;
    debug?: PoseDebug;
}
/** Results of {@link PoseProcessor} */
interface PoseResult {
    /** Array of detected {@link Pose | poses} */
    poses: Pose[];
}
/**
 * Pose processor
 *
 * Pose processor estimates 33 2D and 3D pose keypoints, it locates
 * the person / pose region-of-interest (ROI) and predicts the pose
 * keypoints providing smooth, stable and accurate pose estimation.
 * 2D pixel pose keypoints - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space, it has the same scale
 * as X coordinate (normalized by image width) and 0 is at the center
 * of hips. These points can be used for 2D pose overlays or when
 * using orthographic projection. Estimation of Z coordinate is not
 * very accurate and we recommend to use only XY for 2D effects.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used for 3D avatar overlays or
 * virtual try-on. Rigged and skinned models can be rendered on top
 * of the pose aligning skeleton/armature joints with 3D keypoints.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 * Pose processor may estimate an accurate & stable segmentation mask.
 * Segmentation mask - monochrome image, where every pixel has value
 * in range [0..1] denoting the probability of it being a foreground.
 * Mask is provided for normalized rect region of the original image,
 * it has a fixed size in pixels and should be scaled to image space.
 * Optional temporal smoothing of a segmentation mask may be enabled.
 * Estimated mask may be used for background substitution, effects like
 * bokeh or focal blur, advanced occluder materials utilizing a mask,
 * regional patchers, and other foreground/background shader effects.
 */
declare class PoseProcessor extends Processor<PoseResult, PoseParams> {
    /** Pose tracker (computation engine) */
    private poseTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /** Constructor */
    constructor();
    /**
     * Initialize processor
     *
     * Prepares all resources required for pose estimation.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: PoseParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Pose processor detects pose and predicts pose keypoints.
     *
     * @param input - Image
     * @param timestamp - Image timestamp
     * @returns Pose estimations
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<PoseResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Pose processor fixes FoV for more accurate pose alignment.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Pose engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link PoseProcessor}. Straightforward wrapper,
 * instead of `new Engine(PoseProcessor, ...)` you can
 * use simpler `const engine = new PoseEngine(...})`.
 */
declare class PoseEngine extends Engine<PoseResult, PoseParams, PoseProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}
/** List of pose points */
declare const PosePointNames: readonly ["nose", "eyeInnerL", "eyeL", "eyeOutterL", "eyeInnerR", "eyeR", "eyeOutterR", "earL", "earR", "mouthL", "mouthR", "shoulderL", "shoulderR", "elbowL", "elbowR", "wristL", "wristR", "pinkyL", "pinkyR", "indexL", "indexR", "thumbL", "thumbR", "hipL", "hipR", "kneeL", "kneeR", "ankleL", "ankleR", "heelL", "heelR", "footIndexL", "footIndexR"];
/** Name of pose point */
type PosePointName = typeof PosePointNames[number];
/**
 * Mapped type of keys
 *
 * @typeParam K - Union of string keys
 * @typeParam T - Type of a mapped values
 */
type KeyMap<K extends string[][number], T> = {
    [key in K]: T;
};
/**
 * Map of pose
 *
 * Object with properties corresponding to each pose point.
 *
 * @typeParam T - Type of mapped values
 */
type PoseMap<T> = KeyMap<PosePointName, T>;
/** Pose points */
type PosePoints = PoseMap<PosePoint>;
/**
 * Build a key-value mapped object
 *
 * Constructs a mapped object where values of properties
 * corresponding to each key from the union are evaluated
 * by the provided mapping function called on these keys.
 *
 * @param mapFn - Function mapping a key to a value
 * @typeParam K - Union of string keys
 * @typeParam T - Type of a mapped object values
 * @returns Mapped object of keys
 */
declare function mapKeys<K extends readonly string[], T>(mapFn: (k: K[number]) => T, keys: K): { [key in K[number]]: T; };
/**
 * Mapping of pose points
 *
 * Builds a mapped object where values of properties
 * for selected {@link PosePointNames | pose points}
 * are evaluated by the provided mapping function.
 *
 * @param mapFn - Function mapping a key to a value
 * @typeParam K - Union of string keys
 * @typeParam T - Type of a mapped object values
 * @returns Mapped object of pose points
 */
declare const mapPoints: <P extends readonly PosePointName[], T>(mapFn: (p: P[number]) => T, points: P) => { [key in P[number]]: T; };
/**
* Mapping of pose
*
* Builds a mapped object where values of property
* for each {@link PosePointNames | pose point}
* is evaluated by the provided mapping function.
*
* @param mapFn - Function mapping a key to a value
* @typeParam T - Type of a mapped object values
* @returns Mapped object of pose points
*/
declare const mapPose: <T>(mapFn: (p: PosePointName) => T) => {
    nose: T;
    eyeInnerL: T;
    eyeL: T;
    eyeOutterL: T;
    eyeInnerR: T;
    eyeR: T;
    eyeOutterR: T;
    earL: T;
    earR: T;
    mouthL: T;
    mouthR: T;
    shoulderL: T;
    shoulderR: T;
    elbowL: T;
    elbowR: T;
    wristL: T;
    wristR: T;
    pinkyL: T;
    pinkyR: T;
    indexL: T;
    indexR: T;
    thumbL: T;
    thumbR: T;
    hipL: T;
    hipR: T;
    kneeL: T;
    kneeR: T;
    ankleL: T;
    ankleR: T;
    heelL: T;
    heelR: T;
    footIndexL: T;
    footIndexR: T;
};
/** Index map of points in {@link PosePointNames} */
declare const PosePointIdx: {
    nose: number;
    eyeInnerL: number;
    eyeL: number;
    eyeOutterL: number;
    eyeInnerR: number;
    eyeR: number;
    eyeOutterR: number;
    earL: number;
    earR: number;
    mouthL: number;
    mouthR: number;
    shoulderL: number;
    shoulderR: number;
    elbowL: number;
    elbowR: number;
    wristL: number;
    wristR: number;
    pinkyL: number;
    pinkyR: number;
    indexL: number;
    indexR: number;
    thumbL: number;
    thumbR: number;
    hipL: number;
    hipR: number;
    kneeL: number;
    kneeR: number;
    ankleL: number;
    ankleR: number;
    heelL: number;
    heelR: number;
    footIndexL: number;
    footIndexR: number;
};
/** Pose detection debug data */
type PoseDebug = NonNullable<PoseDetection["debug"]>;

/** Parameters of {@link HandProcessor} */
interface HandParams extends ProcParams {
    /**
     * Detect wrist
     *
     * Wrist detection result is the middle line
     * and two wrist edges provided as normalized
     * anchor points and unit direction vector.
     */
    wrist?: boolean;
}
/** Hand detection */
interface Hand {
    /**
     * List of hand {@link HandPoint | keypoints}
     *
     * 2D pixel coordinate - point in the screen coordinate space.
     * XY coordinates are normalized screen coordinates (scaled by
     * image width and height), while the Z coordinate is depth
     * in orthographic projection space, it has the same scale as X.
     * 3D metric coordinate - point within 3D space of perspective
     * camera located at the space origin and pointed in the negative
     * direction of the Z-axis. 3D & 2D points are perfectly aligned.
     */
    points: HandPoint[];
    /**
     * List of phalanxes
     *
     * Phalanx detection defines the center point and two
     * points of corresponding edges at the same section.
     */
    phalanxes: PhalanxDetection[];
    /**
     * Wrist detection
     *
     * Wrist detection provides 3 lines in screen coordinate space.
     * Middle line defines 2D wrist center and unit direction vector
     * of the wrist. Two wrist edges are defined by 2D points at the
     * end of the wrist along transversal section through the center.
     */
    wrist?: WristDetection;
    /**
     * Classification score of handedness
     *
     * Number between -1 and 1 that represents handedness.
     * Negative value signals right hand, positive - left.
     * Bigger magnitude (absolute value) means more rebust
     * classification, the closer value is to zero - the
     * more ambiguous is distinction of the handedness.
     */
    handedness: number;
    /** Reliability score, number between 0 and 1 */
    score: number;
    debug?: HandDetection["debug"];
}
/** Results of {@link HandProcessor} */
interface HandResult {
    /** Array of detected {@link Hand | hands} */
    hands: Hand[];
}
/**
 * Hand processor
 *
 * Hand processor estimates 21 2D and 3D hand keypoints, it locates
 * the hand region-of-interest (ROI) and predicts the pose keypoints
 * providing smooth, stable and accurate pose estimation fot the hand.
 * 2D pixel hand keypoints - points in the screen coordinate space.
 * X and Y coordinates are normalized screen coordinates (scaled by
 * width and height of the input image), while the Z coordinate is
 * depth within orthographic projection space, it has the same scale
 * as X coordinate (normalized by image width). 2D points can be used
 * for 2D overlays, math analyzes, or when using orthographic camera.
 * 3D metric points - points within 3D space of perspective camera
 * located at the space origin and pointed in the negative direction
 * of the Z-axis. These points can be used for 3D model overlays or
 * virtual try-on. Rigged and skinned models can be rendered on top
 * of the pose aligning skeleton/armature joints with 3D keypoints.
 * 3D and 2D points are perfectly aligned, projections of 3D points
 * coincide with 2D pixel coordinates within the perspective camera.
 * Additionally hand processor detects wrist 2D position and direction.
 * Wrist detection provides 3 lines in the screen coordinate space.
 * Middle line defines 2D wrist base/center point and unit direction
 * vector of the wrist. Two more lines define wrist edges by 2D screen
 * points at the end of the wrist along transversal section through
 * the base point and associated direction vectors. Wrist detection
 * provides for virtual try-on of accessories like watches and bands.
 */
declare class HandProcessor extends Processor<HandResult, HandParams> {
    /** Hand tracker (computation engine) */
    private handTracker;
    /** View angle of perspective camera along smallest side */
    readonly cameraAngleBase: number;
    /**
     * Initialize processor
     *
     * Prepares all resources required for pose estimation.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: HandParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Hand processor detects hand and predicts keypoints.
     *
     * @param input - Image
     * @param timestamp - Image timestamp
     * @returns Hand pose detections
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<HandResult>;
    /**
     * Set resolution of the input video
     *
     * Defines view angle according to resolution and aspect ratio.
     * Hand processor fixes FoV for more accurate hand alignment.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}
/**
 * Hand engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link HandProcessor}. Straightforward wrapper,
 * instead of `new Engine(HandProcessor, ...)` you can
 * use simpler `const engine = new HandEngine(...})`.
 */
declare class HandEngine extends Engine<HandResult, HandParams, HandProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

/** Parameters of {@link MaskProcessor} */
interface MaskParams extends ProcParams {
    /** Enable temporal smoothing of segmentation mask */
    smooth?: boolean;
    /** Evaluate segmentation mask with lower 128x128 resolution */
    masksm?: boolean;
}
/** Segmentation mask */
interface Mask {
    /**
     * Segmentation mask - monochrome image, where every pixel has value
     * from 0 to 1 denoting the probability of it being a foreground pixel.
     * Mask is provided for normalized rect region of the original image,
     * it has a fixed size in pixels and should be scaled to image space.
     */
    mask?: BodyMask;
}
/** Results of {@link MaskProcessor} */
interface MaskResult {
    masks: Mask[];
}
/**
 * Segmentation mask processor
 *
 * Mask processor estimates accurate & stable human segmentation masks.
 * Segmentation mask - monochrome image, where every pixel has value
 * in range [0..1] denoting the probability of it being a foreground.
 * Mask is provided for normalized rect region of the original image,
 * it has a fixed size in pixels and should be scaled to image space.
 * Optional temporal smoothing of a segmentation mask may be enabled.
 * Estimated mask may be used for background substitution, effects like
 * bokeh or focal blur, advanced occluder materials utilizing a mask,
 * regional patchers, and other foreground/background shader effects.
 */
declare class MaskProcessor extends Processor<MaskResult, MaskParams> {
    /** Segmentation mask tracker (computation engine) */
    private maskTracker;
    /**
     * Initialize processor
     *
     * Prepares all resources required for face mesh tracking.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: MaskParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Mask processor evaluates and tracks segmentation masks.
     *
     * @param input - Image
     * @param timestamp - Image timestamp
     * @returns Segmentation masks
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<MaskResult>;
}
/**
 * Mask engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link MaskProcessor}. Straightforward wrapper,
 * instead of `new Engine(MaskProcessor, ...)` you can
 * use simpler `const engine = new MaskEngine(...})`.
 */
declare class MaskEngine extends Engine<MaskResult, MaskParams, MaskProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

/**
 * Hair segmentation mask processor
 *
 * Hair processor estimates accurate & stable hair segmentation masks.
 * Segmentation mask - monochrome image, where every pixel has value
 * in range [0..1] denoting the probability of it being a foreground.
 * Mask is provided for normalized rect region of the original image,
 * it has a fixed size in pixels and should be scaled to image space.
 * Optional temporal smoothing of a segmentation mask may be enabled.
 * Estimated mask can be used for hair recoloring, color estimation,
 * advanced occluder materials utilizing a mask, regional patchers.
 */
declare class HairProcessor extends Processor<MaskResult, MaskParams> {
    /** Hair segmentation tracker (computation engine) */
    private hairTracker;
    /**
     * Initialize processor
     *
     * Prepares all resources required for face mesh tracking.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @override
     */
    init(params: MaskParams, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     *
     * @override
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     *
     * @override
     */
    dispose(): void;
    /**
     * Process the image
     *
     * Mask processor evaluates and tracks segmentation masks.
     *
     * @param input - Image
     * @param timestamp - Image timestamp
     * @returns Segmentation masks
     * @override
     */
    process(input: ImageInput, timestamp?: number): Promise<MaskResult>;
}
/**
 * Hair engine
 *
 * Specialization of {@link @geenee/armature!Engine}
 * for {@link HairProcessor}. Straightforward wrapper,
 * instead of `new Engine(HairProcessor, ...)` you can
 * use simpler `const engine = new HairEngine(...})`.
 */
declare class HairEngine extends Engine<MaskResult, MaskParams, HairProcessor> {
    /**
     * Constructor
     *
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     */
    constructor(engineParams?: EngineParams, Source?: new () => VideoSource);
}

export { type Face, FaceEngine, type FaceParams, FaceProcessor, type FaceResult, HairEngine, HairProcessor, type Hand, HandEngine, type HandParams, HandProcessor, type HandResult, type KeyMap, type Mask, MaskEngine, type MaskParams, MaskProcessor, type MaskResult, type Pose, PoseEngine, type PoseMap, type PoseParams, PosePointIdx, type PosePointName, PosePointNames, type PosePoints, PoseProcessor, type PoseResult, mapKeys, mapPoints, mapPose };
