type Args<F> = [F] extends [(...args: infer U) => any] ? U : [F] extends [void] ? [] : [F];
/**
 * EventEmitter class
 *
 * These objects expose an on() function that allows one or more
 * functions to be attached to named events emitted by the object.
 * When the EventEmitter object emits an event, all of the functions
 * attached to that specific event are called synchronously. Any
 * values returned by the called listeners are ignored and discarded.
 *
 * @typeParam Events - Events emitted by EventEmitter
 */
interface EventEmitterTI<Events> {
    /**
     * Adds the listener function to the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    on<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds a one-time listener function for the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    once<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes the listener from the listener array for the event
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    off<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds the listener function to the end of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    addListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds the listener function to the beginning of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    prependListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Adds a one-time listener function to the beginning of the listeners array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     */
    prependOnceListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes the specified listener from the listener array
     *
     * @param event - The name of the event
     * @param listener - The callback function
     * @returns This EventEmitter
     */
    removeListener<E extends keyof Events>(event: E, listener: Events[E]): this;
    /**
     * Removes all listeners, or those of the specified event
     *
     * @param event - The name of the event
     * @returns This EventEmitter
     */
    removeAllListeners<E extends keyof Events>(event?: E): this;
    /**
     * Synchronously calls each of the listeners registered for the event
     *
     * @param event - The name of the event
     * @param args - Arguments passed to the listeners
     * @returns True if the event had listeners, False otherwise
     */
    emit<E extends keyof Events>(event: E, ...args: Args<Events[E]>): boolean;
    /**
     * The number of listeners listening to the event
     *
     * @param event - The name of the event
     * @returns Number of listeners
     */
    listenerCount<E extends keyof Events>(event: E): number;
    /**
     * Copy of the array of listeners for the event
     *
     * @param event - The name of the event
     * @returns Copy of the listeners array
     */
    listeners<E extends keyof Events>(event: E): Function[];
    /**
     * Copy of the array of listeners for the event including wrappers
     *
     * @param event The name of the event
     * @returns Copy of the listeners array
     */
    rawListeners<E extends keyof Events>(event: E): Function[];
    /**
     * Sets maximum number of listeners per event
     *
     * @param n - Maximum number of listeners
     * @returns This EventEmitter
     */
    setMaxListeners(n: number): this;
    /**
     * Maximum number of listeners per event
     * @returns Maximum number of listeners per event
     */
    getMaxListeners(): number;
    /**
     * List of emitter's events
     * @returns List of emitter's events
     */
    eventNames(): (keyof Events | string | symbol)[];
}
declare const EventEmitterT_base: new <Events_1>() => EventEmitterTI<Events_1>;
/**
 * EventEmitter generic class
 *
 * These objects expose an on() function that allows one or more
 * functions to be attached to named events emitted by the object.
 * When the EventEmitter object emits an event, all of the functions
 * attached to that specific event are called synchronously. Any
 * values returned by the called listeners are ignored and discarded.
 *
 * @typeParam Events - Events emitted by EventEmitter
 */
declare class EventEmitterT<Events> extends EventEmitterT_base<Events> {
}

/** Size */
interface Size {
    /** Width */
    width: number;
    /** Height */
    height: number;
}
/** Image input types (supported) */
type ImageInput = ImageData | ImageBitmap | HTMLCanvasElement;

/** Basic processor parameters */
interface ProcParams {
    /**
     * The SDK access token. Mandatory parameter authenticating
     * your user account and providing access to the SDK on the
     * current url. You can create tokens for required urls on
     * your [account page](https://builder.geenee.ar/sdk). Token
     * must be provided to initialize {@link Engine#init | Engine}.
     * [More](https://engeenee.com/guides/intro/).
     */
    token: string;
    /**
     * Root path to computational modules (statically served wasms).
     * The SDK requires access to wasm modules provided within its
     * packages. By default, the root path is the current url "./".
     * Root can be set on {@link Engine#init | Engine initialization}.
     */
    root?: string;
    /** Cache computational modules (experimental) */
    cache?: boolean;
}
/** Events emitted by {@link Processor} */
interface ProcessorEvents {
    /** Processor initialized */
    init: (status: boolean) => void;
    /** Processor reset */
    reset: () => void;
}
/**
 * Core generic processor
 *
 * Processor is a computational core of any application and the
 * most essential part of the {@link Engine} in an app's pipeline.
 * Processing results are used by {@link Renderer} to update scene.
 * Every processor must define methods to initialize and release
 * instances required for image processing, and evaluation of
 * processing results on provided image (where all logic happens).
 * Processor is a generic abstract class defining common API.
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam ParamsT - Type of processor parameters
 */
declare class Processor<ResultT extends {} = {}, ParamsT extends ProcParams = ProcParams> extends EventEmitterT<ProcessorEvents> {
    /** Processor parameters */
    protected params: Partial<ParamsT>;
    /** Resolution of input video */
    protected videoSize: Size;
    /** Aspect ratio of input video */
    protected videoRatio: number;
    /** Recommended maximum size of input */
    optimalSize: number;
    /** Camera aspect ratio */
    cameraRatio: number;
    /** Camera vertical angle in radians */
    cameraAngle: number;
    /** Constructor */
    constructor();
    /**
     * Process the image
     *
     * Main method defining the logic of video processing.
     * Overridden by derived classes for particular application.
     *
     * @param input - Image
     * @param timestamp - Timestamp
     * @virtual
     */
    process(input: ImageInput, timestamp?: number): Promise<ResultT>;
    /**
     * Initialize processor
     *
     * Initializes all resources required for video processing.
     * Overridden by derived classes for particular application.
     *
     * @param params - Processor parameters
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @returns Status of initialization
     * @virtual
     */
    init(params: ParamsT, size?: Size, ratio?: number): Promise<boolean>;
    /**
     * Reset processor
     *
     * Resets all processing instances to the initial state.
     * Overridden by derived classes for particular processing.
     *
     * @virtual
     */
    reset(): void;
    /**
     * Dispose processor object
     *
     * Releases resources and instances allocated by processor.
     * Processor object cannot be used after calling dispose().
     * Overridden by derived classes for particular processing.
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Set resolution of the input video
     *
     * Could be overridden to adjust processing pipeline.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @virtual
     */
    setupVideo(size: Size, ratio?: number): void;
}

/** Events emitted by {@link Renderer} */
interface RendererEvents {
    /** Renderer initialized */
    load: () => void;
    /** Scene updated, emitted on every iteration */
    render: () => void;
    /** Rendering canvas resized */
    resize: (size: Size, ratio: number) => void;
}
/**
 * Core generic renderer
 *
 * Renderer is the core visualization part of any application.
 * It's attached to {@link Engine}. Results of processing and
 * captured frame are provided to renderer that updates scene,
 * visualization and application logic according to this data.
 * Basically, renders define two methods load() and update().
 * The first one is used to initialize all assets and prepare
 * the scene e.g. set up lightning, environment map. Engine
 * will call load() method during pipeline initialization.
 * The second one is used to update the scene using results
 * of video processing. This's where all the logic happens.
 * By overriding/extending {@link Renderer#load | load()} and
 * {@link Renderer#update | update()} you can add any custom
 * logic, interactions, animations, post-processing effects,
 * gesture recognition, physics, etc. to an app. Renderer is
 * a generic abstract class defining common core interfaces.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class Renderer<ResultT> extends EventEmitterT<RendererEvents> {
    /** Loaded state */
    protected loaded: boolean;
    /** Resolution of input video */
    protected videoSize: Size;
    /** Aspect ratio of input video */
    protected videoRatio: number;
    /** Camera aspect ratio */
    protected cameraRatio: number;
    /** Camera vertical angle in radians */
    protected cameraAngle: number;
    /** Constructor */
    constructor();
    /**
     * Initialize renderer
     *
     * Initializes renderer, all required assets, and the scene.
     * Overridden by derived classes for particular application.
     *
     * @returns Promise resolving when initialization is finished
     * @virtual
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Releases all resources and instances created in load().
     * Overridden by derived classes for particular application.
     *
     * @virtual
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Main method defining the logic of the renderer.
     * Updates the scene according to provided results.
     * Overridden by derived classes for the application.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @virtual
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update the video layer
     *
     * Virtual method drawing input video frame.
     * Derived renderer provides implementation.
     *
     * @param stream - Captured video frame
     * @virtual
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Update and render the scene
     *
     * Virtual method updating and rendering the scene.
     * Overridden by implementation of derived renderer.
     *
     * @virtual
     */
    protected updateScene(): void;
    /**
     * Dispose renderer object
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Set video parameters
     *
     * Could be overridden to adjust rendering pipeline.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @virtual
     */
    setupVideo(size: Size, ratio?: number): void;
    /**
     * Set camera parameters
     *
     * Some video processors statically define camera parameters
     * from just a video resolution. In this cases setupCamera()
     * is used to pass these static camera parameters to renderer.
     *
     * @param ratio - Camera aspect ratio
     * @param angle - Camera vertical angle in radians
     * @virtual
     */
    setupCamera(ratio: number, angle: number): void;
}

/**
 * Image buffer
 *
 * Helper class grabbing images into internal storage.
 * Used by {@link VideoSource} to grab video streams and
 * {@link Engine} to resize images for faster processing.
 * Image can be accessed by canvas or read as pixels.
 */
declare class ImageBuffer {
    /** Size of the frames */
    size: Size;
    /** Clipping region in normalized coordinates */
    protected clip?: ClipRect;
    /** Clipping region in pixels */
    protected clipPix?: ClipRect;
    /** Transpose image */
    protected transpose: boolean;
    /** Canvas to capture frames */
    canvas: HTMLCanvasElement;
    /** Context to capture frames */
    protected context: CanvasRenderingContext2D | null;
    /**
     * Constructor
     *
     * @param name - Optional name of canvas
     */
    constructor(name?: string);
    /**
     * Grab frame
     *
     * Grabs frame from video element or canvas into internal canvas.
     * Automatically rescales an image if resolutions are different.
     *
     * @param video - Element to grab image from
     * @returns True if image is grabbed, False otherwise
     */
    capture(video: HTMLVideoElement | HTMLCanvasElement | ImageBitmap): boolean;
    /** Fill canvas with color */
    fill(): void;
    /**
     * Get image buffer
     *
     * Returns data buffer of image currently grabbed into canvas.
     *
     * @returns Image data buffer on success, undefined otherwise
     */
    data(): ImageData | undefined;
    /**
     * Set input video size
     *
     * @param size - Video size
     */
    setSize(size: Size): void;
    /**
     * Set normalized clipping rectangle
     *
     * @param clip - Clipping rectangle in [0..1] space
     */
    setClip(clip?: ClipRect): void;
    /**
     * Set image transposition
     *
     * @param transpose - Transpose image flag
     */
    setTranspose(transpose: boolean): void;
    /** Dispose video context object */
    dispose(): void;
}
/** Parameters of video capture */
interface VideoParams {
    /** Size requested from a camera */
    size?: Size;
    /** Request rear-facing camera if presented */
    rear?: boolean;
    /** Request frames per second if supported */
    fps?: number;
    /** Request additional options */
    opts?: MediaStreamConstraints;
    /** Request aspect ratio */
    ratio?: number;
    /** Request clipping */
    clip?: ClipRect;
    /** Request transpose */
    transpose?: boolean;
}
/** Parameters of file capture */
interface FileParams {
    /** Video file url */
    url: string;
    /** Maximum capture size */
    sizeMax?: number;
}
/** Setup parameters of video capture */
type VideoSourceParams = VideoParams | MediaStream | FileParams | string;
/** Clip rectangle */
interface ClipRect {
    /** Left */
    left: number;
    /** Top */
    top: number;
    /** Width */
    width: number;
    /** Height */
    height: number;
}
/** Events emitted by {@link VideoCapture} */
interface CaptureEvents {
    /** Video resize */
    resize: (size: Size) => void;
}
/**
 * Video source
 *
 * General class of video capture objects providing
 * functionality to grab images from various sources.
 * It implements basic interfaces like setup, start,
 * stop of the video capture, and grabbing of frames.
 * Internally utilizes {@link ImageBuffer} as storages.
 */
declare class VideoSource extends EventEmitterT<CaptureEvents> {
    /** Context of original stream */
    buffer: ImageBuffer;
    /** Timestamp of the last captured frame */
    captureTime: number;
    /** Maximum capture resolutions */
    protected sizeMax?: number;
    /** Aspect ratio */
    protected aspectRatio?: number;
    /** Clipping rectangle */
    protected clipRect?: ClipRect;
    /** Transpose frames */
    protected transpose?: boolean;
    /** Timer to emulate timestamps */
    private timer?;
    /** Constructor */
    constructor();
    /**
     * Setup video source
     *
     * Sets up capture, overridden for particular video source.
     * Video source can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution.
     * Additional fine-grained MediaStreamConstraints can be passed
     * via `opts` field providing the most flexible way to setup
     * video stream (for example, by requesting specific deviceId).
     * Stream constraints have higher priority than rest or params.
     * Another options are an external MediaStream allowing custom
     * video sources (e.g. remote stream) or a media file defined
     * by url or {@link FileParams} that allows to limit maximum
     * capture size in cases resolution is too big for a device.
     * Default implementation sets video resolution according to
     * size in {@link VideoParams} or 1920x1080 as the fallback.
     * It captures static image, canvas is filled by white color.
     *
     * @param params - Parameters of video capture
     * @returns Promise resolved to the status of setup when done
     * @virtual
     */
    setup(params?: VideoSourceParams): Promise<boolean>;
    /**
     * Dispose video source object
     *
     * @virtual
     */
    dispose(): void;
    /**
     * Start video capture
     *
     * Video capture can be started only after successful setup().
     *
     * @returns Promise resolved when capture is started
     * @virtual
     */
    start(): Promise<void>;
    /**
     * Pause video capture
     *
     * @virtual
     */
    pause(): void;
    /**
     * Reset video capture
     *
     * After reset() capture may be started again only after setup().
     *
     * @virtual
     */
    reset(): void;
    /**
     * Grab the next video frame
     *
     * VideoSource grabs the static image permanently
     * stored in embedded {@link ImageBuffer} object.
     *
     * @returns True if next frame was available and grabbed
     * @virtual
     */
    capture(): boolean;
    /**
     * Resolution of the video stream
     *
     * @returns Resolution of the video stream
     */
    size(): Size;
    /**
     * Aspect ratio of the video stream
     *
     * @returns Aspect ratio of the video stream
     */
    ratio(): number;
    /**
     * Update callback on video resize
     *
     * @param size - New size of the video stream
     */
    protected updateSize(size: Size): void;
}

/** Parameters of Engine */
interface EngineParams {
    /** Maximum processing size (resizing). If not defined
     * engine uses image size preferred by processor. */
    max?: number;
    /** Preserve original resolution, default - true. */
    orig?: boolean;
}
/** Events emitted by {@link Engine} */
interface EngineEvents {
    /** Engine initialized */
    init: (status: boolean) => void;
    /** Engine set up */
    setup: (status: boolean) => void;
    /** Pipeline started */
    start: () => void;
    /** Pipeline stopped */
    pause: () => void;
}
/**
 * Core generic engine
 *
 * Engine is a core of any app and organizer of a pipeline.
 * It's responsible to interact with lower-level instances and
 * at the same time provide simple and user-friendly interface.
 * Engine combines together data (video) capturing, processing
 * and rendering. It's created for particular {@link Processor}.
 * Processor's constructor or instance is provided to the engine
 * and former initializes and controls state of processor during
 * life-circle of the application. Results of processing and
 * captured image are passed to a {@link Renderer} attached to
 * the Engine. Notable feature is fast rescaling of images for
 * processing down to the requested resolution in cases when
 * available camera output is bigger. Engine parameters have an
 * option to limit processed image size. Original video stream
 * can be preserved on request, this is useful when processing
 * cannot handle high resolution images but you still want to
 * render high quality video in your app. All core components:
 * Engine, Processor, and Renderer are generics parametrized by
 * type of processing results and optionally tuning parameters.
 *
 * @typeParam ProcessorT - Type of processor
 * @typeParam ResultT - Type of processing results
 * @typeParam ParamsT - Type of processor parameters
 */
declare class Engine<ResultT extends {}, ParamsT extends ProcParams, ProcessorT extends Processor<ResultT, ParamsT>> extends EventEmitterT<EngineEvents> {
    protected engineParams?: EngineParams | undefined;
    /** Processor utilized by the engine */
    protected processor: ProcessorT;
    /** Renderer attached to the engine */
    protected renderers: Renderer<ResultT>[];
    /** Video source instance */
    protected video: VideoSource;
    /** Ratio of video stream */
    protected videoRatio: number;
    /** Shallow copy of canvas with video for renderers */
    protected streamCanvas?: HTMLCanvasElement;
    /** Size of video for renderers */
    protected streamSize: Size;
    /** Shallow copy of canvas with video for processors */
    protected processCanvas?: HTMLCanvasElement;
    /** Size of video for processors */
    protected processSize: Size;
    /** Buffer to resize frames */
    protected resizeBuffer?: ImageBuffer;
    /** Original stream is resized */
    protected resizeEnabled: boolean;
    /** State of the pipeline */
    private loopState;
    /** Id of current frame */
    private loopId?;
    /**
     * Constructor
     *
     * @param Processor - Processor class or instance
     * @param engineParams - Parameters of the engine
     * @param Source - Video source class or instance
     * @typeParam ProcessorT - Type of processor
     * @typeParam ResultT - Type of processing results
     * @typeParam ParamsT - Type of processor parameters
     */
    constructor(Processor: (new () => ProcessorT) | ProcessorT, engineParams?: EngineParams | undefined, Source?: (new () => VideoSource) | VideoSource);
    /**
     * Initialize engine. Sets up processor.
     *
     * The SDK {@link ProcParams#token | access token} is
     * required parameter that authenticates the user and
     * enables the SDK on the current url. By default, path
     * to required wasm modules provided with SDK packages
     * is the current url. You can change the root path to
     * wasms passing {@link ProcParams#root | root parameter}.
     *
     * @param procParams - Parameters of the processor
     * @returns Status of initialization
     */
    init: (procParams: ParamsT) => Promise<boolean>;
    /**
     * Setup engine. Initializes video capture.
     *
     * Video capture can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution.
     * Additional fine-grained MediaStreamConstraints can be passed
     * via `opts` field providing the most flexible way to setup
     * video stream (for example, by requesting specific deviceId).
     * Stream constraints have higher priority than rest or params.
     * Another options are an external MediaStream allowing custom
     * video sources (e.g. remote stream) or a media file defined
     * by url or {@link FileParams} that allows to limit maximum
     * capture size in cases resolution is too big for a device.
     *
     * @param videoParams - Parameters of video capture
     * @returns Status of initialization
     */
    setup: (videoParams?: VideoSourceParams) => Promise<boolean>;
    /**
     * Start pipeline.
     *
     * Pipeline can be started only after successful init and setup.
     */
    start: () => Promise<void>;
    /**
     * Pause pipeline.
     *
     * Nothing happens if pipeline is not started yet.
     */
    pause: () => void;
    /**
     * Reset pipeline
     *
     * Stops pipeline, resets video capture and processor.
     * After reset one needs to reinitialize video capture
     * calling setup() before pipeline can be started again.
     */
    reset: () => void;
    /**
     * Attach Renderer to the engine
     *
     * @param renderer - Object to be attached
     */
    addRenderer(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Remove attached Renderer
     *
     * @param renderer - Renderer to be removed
     */
    removeRenderer(renderer: Renderer<ResultT>): void;
    /** Iterate */
    protected iterate: () => Promise<void>;
    /** Enqueue the next iteration */
    protected enqueue(): void;
    /** Setup processor */
    protected setupProcessor(procParams: ParamsT): Promise<boolean>;
    /** Setup video capture */
    protected setupVideo(videoParams?: VideoSourceParams): Promise<boolean>;
    /** Setup video size */
    protected setupSize(size: Size): Promise<void>;
    /**
     * Callback called when video resolution is changed
     *
     * @param size - Size of the video
     */
    protected resizeVideo(size: Size): void;
}

/**
 * Video capture
 *
 * VideoCapture object provides means to grab images from a source.
 * It implements all required functionality including video device
 * initialization, setup, start/stop of capture and frame grabbing.
 * Internally VideoCapture utilizes {@link ImageBuffer} as storage.
 */
declare class VideoCapture extends VideoSource {
    /** Video element */
    protected videoRef: HTMLVideoElement;
    /** Time shift for video loop */
    private timeShift;
    /** Constructor */
    constructor();
    /**
     * Setup video capture
     *
     * Sets up video device, streams and contexts with canvases.
     * Video capture can be setup by simplified {@link VideoParams}
     * opening default front/rear camera with provided resolution.
     * Additional fine-grained MediaStreamConstraints can be passed
     * via `opts` field providing the most flexible way to setup
     * video stream (for example, by requesting specific deviceId).
     * Stream constraints have higher priority than rest or params.
     * Another options are an external MediaStream allowing custom
     * video sources (e.g. remote stream) or a media file defined
     * by url or {@link FileParams} that allows to limit maximum
     * capture size in cases resolution is too big for a device.
     *
     * @param params - Parameters of video capture
     * @returns Promise resolved to the status of setup when finished
     */
    setup(params?: VideoSourceParams): Promise<boolean>;
    /**
     * Dispose video capture object
     *
     * @override
     */
    dispose(): void;
    /**
     * Start video capture
     *
     * Video capture can be started only after successful setup().
     *
     * @returns Promise resolved when capture is started
     */
    start(): Promise<void>;
    /**
     * Pause video capture
     *
     * @override
     */
    pause(): void;
    /**
     * Reset video capture
     *
     * After reset() capture may be started again only after setup().
     *
     * @override
     */
    reset(): void;
    /**
     * Grab the next video frame
     *
     * VideoCapture grabs the next image from a video source
     * (camera) and stores it in embedded {@link ImageBuffer}.
     *
     * @returns True if next frame was available and grabbed
     * @override
     */
    capture(): boolean;
}

/**
 * Image capture
 *
 * ImageCapture grabs frames from a static source (image).
 * It implements source initialization, setup, start/stop
 * of capture, and frame grabbing. Internally, ImageCapture
 * utilizes {@link ImageBuffer} as storage and fast resizer.
 */
declare class ImageCapture extends VideoSource {
    /** Image bitmap */
    protected imageRef?: ImageBitmap;
    /**
     * Setup video capture
     *
     * Loads image specified by a url and sets up a video capture.
     *
     * @param params - Parameters of video capture
     * @returns Promise resolved to the status of setup when finished
     */
    setup(params?: VideoSourceParams): Promise<boolean>;
    /**
     * Dispose video capture object
     *
     * @override
     */
    dispose(): void;
    /**
     * Start video capture
     *
     * Video capture can be started only after successful setup().
     *
     * @returns Promise resolved when capture is started
     */
    start(): Promise<void>;
    /**
     * Pause video capture
     *
     * @override
     */
    pause(): void;
    /**
     * Reset video capture
     *
     * After reset() capture may be started again only after setup().
     *
     * @override
     */
    reset(): void;
    /**
     * Grab the next video frame
     *
     * VideoCapture grabs the next frame from an image
     * and stores it in embedded {@link ImageBuffer}.
     *
     * @returns True if next frame was available and grabbed
     * @override
     */
    capture(): boolean;
}

/** Events emitted by {@link ResponsiveCanvas} */
interface CanvasEvents {
    /** Resize event */
    resize: () => void;
}
/** {@link ResponsiveCanvas} fitting modes */
type CanvasMode = "crop" | "fit" | "pad";
/**
 * Responsive canvas
 *
 * Responsive canvas is useful helper for applications
 * doing rendering. It creates canvas layers within any
 * html element. Layers preserve aspect ratio according
 * to fitting mode. Ratio of canvas usually follows the
 * ratio of the input video. In "crop" mode canvases are
 * cropped to have the same aspect ratio as the container
 * and scaled to fill the whole area. This is default
 * mode preferred in most cases. In "fit mode" canvases
 * are scaled down to fit into the container and leaving
 * margins to stay in the center, "pad" mode behavior is
 * the same, but margins are filled with highly blurred
 * parts of the input video instead of still background.
 * ResponsiveCanvas tracks size changes of container and
 * updates layers and margins to preserve aspect ratio.
 */
declare class ResponsiveCanvas extends EventEmitterT<CanvasEvents> {
    protected container: HTMLElement;
    protected mode: CanvasMode;
    protected layerCount: number;
    protected mirror: boolean;
    protected aspectRatio: number;
    /** Canvas layers */
    layers: HTMLCanvasElement[];
    /** Padding canvases */
    pads?: [HTMLCanvasElement, HTMLCanvasElement];
    /** Relative size of padding canvases */
    padsSize: [number, number];
    /** Resize observer */
    protected observer?: ResizeObserver;
    /**
     * Constructor
     *
     * @param container - Container of responsive canvas
     * @param mode - Fitting mode
     * @param layerCount - Number of canvas layers
     * @param mirror - Mirror the output
     * @param aspectRatio - Target aspect ratio
     */
    constructor(container: HTMLElement, mode?: CanvasMode, layerCount?: number, mirror?: boolean, aspectRatio?: number);
    /**
     * Set aspect ratio
     *
     * Responsive canvas will preserve provided aspect ratio and
     * resize itself within container according to fitting mode.
     *
     * @param ratio - Aspect ratio
     */
    setAspectRatio: (ratio: number) => void;
    /**
     * Set mirror mode
     *
     * ResponsiveCanvas mirrors the output layers.
     *
     * @param mirror - Mirror the output
     */
    setMirror: (mirror: boolean) => void;
    /**
     * Get mirror mode
     *
     * @returns Current mirror mode
     */
    getMirror: () => boolean;
    /**
     * Set fitting mode
     *
     * Canvas layers preserve aspect ratio according to
     * fitting mode, by cropping, fitting, or padding.
     *
     * @param mode - Fitting mode
     */
    setMode: (mode: CanvasMode) => void;
    /**
     * Get fitting mode
     *
     * @returns Current fitting mode
     */
    getMode: () => CanvasMode;
    /**
     * Dispose responsive canvas
     *
     * Destructs all canvas layers and other instances
     * created by responsive canvas. Object must not
     * be used after dispose() method has been called.
     */
    dispose: () => void;
    /**
     * Update canvas sizes to preserve aspect ratio
     *
     * Updates relative sizes of canvas within container to
     * preserve fixed aspect ratio according to fitting mode.
     * Called when container is resized (by resize observer).
     *
     * @param containerRatio - Aspect ratio of the container
     */
    protected updateSizes: (containerRatio: number) => void;
    /**
     * Resize event callback
     *
     * Callback attached to resize observer to handle size updates.
     * Basically, it calls updateSizes() to adjust canvas sizes.
     *
     * @param entries - Elements being resized
     */
    protected handleResize: (entries: ResizeObserverEntry[]) => void;
}

/**
 * Generic renderer plugin
 *
 * Plugin can be attached to a {@link PluginRenderer} instance.
 * Usually they perform simple tasks that can be separated from
 * bigger app context into atomic building blocks, for example
 * control object on a scene to follow (be attached to) user's
 * head, apply image effect (smoothing, beautification), recognize
 * gestures or poses, notify about state changes or perform other
 * kinds of transformations, pre/post-processing, or analyzes with
 * a 3D scene, video stream or raw data from a {@link Processor}.
 * Plugin is a abstraction level to single out ready-made helpers
 * that can be reused as atomic building blocks of an application.
 * Plugins are very similar to Renderer and also should implement
 * two basic methods load() and update(). Renderer initializes a
 * plugin calling load() and providing itself as argument, plugin
 * in turn remembers renderer it's attached to and gets access to
 * required resources of the renderer, for example 2d or webgl
 * context of a canvas or reference to a 3d scene. In update()
 * method plugins implement actual logic given processing results.
 * Renderer applies attached plugins in order defined by their
 * ordinal number. In some cases priority of plugins execution is
 * important, e.g. there can be a plugin that filters results of
 * pose processing by some constraint, let's say it passes only
 * poses where upper body is in the field of view and asks a user
 * to step back for better virtual try-on experience, this plugin
 * should update poses before plugin that renders virtual apparel.
 *
 *
 * @typeParam ResultT - Type of processing results
 */
declare class Plugin<ResultT extends {} = {}> {
    /** Renderer loaded the plugin */
    protected renderer?: Renderer<ResultT>;
    /** Loaded state */
    loaded: boolean;
    /** Ordinal number */
    ordinal: number;
    /**
     * Initialize plugin
     *
     * Initializes resources/instances needed by plugin.
     * Overridden by derived classes for particular task.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @virtual
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources/instances created in load().
     * Overridden by derived classes for particular task.
     *
     * @virtual
     */
    unload(): void;
    /**
     * Update
     *
     * Main method implementing the logic of the plugin.
     * Overridden by derived classes for particular task.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @virtual
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /** Dispose video plugin */
    dispose(): void;
    /**
     * Set video size
     *
     * Could be overridden to adjust plugin's pipeline.
     *
     * @param size - Resolution of input video
     * @virtual
     */
    setupVideo(size: Size): void;
    /**
     * Set camera parameters
     *
     * Could be overridden to adjust plugin's pipeline.
     *
     * @param ratio - Aspect ration of input video
     * @param angle - Vertical field of view in radians
     * @virtual
     */
    setupCamera(ratio: number, angle: number): void;
}
/** Enumeration of standard plugin ordinals */
declare enum PluginOrdinal {
    PreProcess = 100,
    Process = 200,
    PostProcess = 300,
    PreRender = 700,
    Render = 800,
    PostRender = 900
}

/**
 * Generic plugin renderer
 *
 * Extends {@link Renderer} implementing render plugin system.
 * Plugins can be attached to an instance of the PluginRenderer.
 * Usually they perform simple tasks that can be separated from
 * bigger app context into atomic building blocks, for example
 * control object on a scene to follow (be attached to) user's
 * head, apply image effect (smoothing, beautification), recognize
 * gestures or poses, notify about state changes or perform other
 * kinds of transformations, pre/post-processing, or analyzes with
 * a 3D scene, video stream or raw data from a {@link Processor}.
 * {@link Plugin} is a abstraction level to single out ready-made
 * helpers that can be reused as atomic building blocks of an app.
 * Plugins are very similar to Renderer but do only one task, they
 * also should implement two basic methods load() and update().
 * PluginRenderer initializes all attached plugins calling their
 * load() method and providing itself as an argument for plugin
 * to acquire required resources, for example canvas context or
 * reference to 3d scene. Every rendering cycle PluginRenderer
 * calls update() of all attached and successfully loaded plugins
 * passing results of video processing and current video frame.
 * Plugins are ordered depending on processing or rendering stage
 * they should step in, this order is defined by plugin ordinal
 * number. For example there can be a plugin that filters results
 * of processing by some constraint, let's say it accepts only
 * poses where upper body is in the field of view and asks a user
 * to step back for better virtual try-on experience, this plugin
 * should update poses before plugin that renders virtual apparel.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class PluginRenderer<ResultT extends {} = {}> extends Renderer<ResultT> {
    /** Attached plugins */
    protected plugins: Plugin<ResultT>[];
    /**
     * Initialize renderer
     *
     * Initializes all attached plugins.
     *
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Resets all attached plugins.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Updates all attached plugins.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update attached plugins
     *
     * Calls update() of all attached plugins.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     */
    protected updatePlugins(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Dispose renderer object
     *
     * Extended to dispose all attached plugins.
     *
     * @override
     */
    dispose(): void;
    /**
     * Add render plugin
     *
     * Initializes the plugin if it's not loaded yet but renderer is ready.
     * Renderer takes ownership of the plugin instance meaning it will
     * release it when plugin is detached or renderer is disposed itself.
     */
    addPlugin(plugin: Plugin<ResultT>): Promise<void>;
    /**
     * Remove render plugin
     *
     * Renderer will dispose the plugin before detaching it.
     */
    removePlugin(plugin: Plugin<ResultT>): void;
    /**
     * Remove all render plugins
     *
     * Renderer will dispose all plugins before detaching them.
     */
    removeAllPlugins(): void;
    /**
     * Set video parameters
     *
     * Callback sets up all attached plugins.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
    /**
     * Set camera parameters
     *
     * Callback sets up all attached plugins.
     *
     * @param ratio - Camera aspect ratio
     * @param angle - Camera vertical angle in radians
     * @override
     */
    setupCamera(ratio: number, angle: number): void;
}

/** Parameters of {@link CanvasRenderer} */
interface CanvasParams {
    /** Container of responsive canvas */
    container: HTMLElement;
    /** Fitting mode */
    mode?: CanvasMode;
    /** Number of canvas layers */
    layerCount?: number;
    /** Mirror the output */
    mirror?: boolean;
    /** Target aspect ratio */
    aspectRatio?: number;
}
/**
 * Generic renderer using {@link ResponsiveCanvas}
 *
 * Generic {@link Renderer} utilizing {@link ResponsiveCanvas}.
 * Refer to their documentation for more details. CanvasRenderer
 * can have several layers and there're two basic usage patterns.
 * Use separate layers for video and scene and effectively render
 * scene on top of the video stream. Advantage of this approach
 * is that image and scene can be processed independently and one
 * can apply different effects or postprocessing. This pattern is
 * also easier to implement. Or one can use only one canvas layer
 * and embed video stream into the scene as object via a texture
 * or background component. This approach will have more complex
 * implementation dependent on particular renderer. On the other
 * hand, rendering effects affecting the whole scene will also
 * apply to the video stream. This can improve performance and
 * allows advanced rendering/postprocessing techniques to be used.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class CanvasRenderer<ResultT extends {} = {}> extends PluginRenderer<ResultT> {
    /** Responsive canvas */
    canvas: ResponsiveCanvas;
    /** Drawing context of padding canvases */
    protected padCtx: [
        CanvasRenderingContext2D | null,
        CanvasRenderingContext2D | null
    ];
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Update the video layer
     *
     * Updates padding canvases using portions of video frame.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Dispose renderer object
     *
     * Extended to dispose responsive canvas
     *
     * @override
     */
    dispose(): void;
    /**
     * Set video parameters
     *
     * Callback passes aspect ratio to responsive canvas.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
    /**
     * Set mirror mode
     *
     * CanvasRenderer sets mirror mode of ResponsiveCanvas.
     *
     * @param mirror - Mirror the output
     */
    setMirror(mirror: boolean): void;
    /**
     * Set fitting mode
     *
     * CanvasRenderer sets fitting mode of ResponsiveCanvas.
     *
     * @param mode - Fitting mode
     */
    setMode(mode: CanvasMode): void;
    /**
     * Update padding canvases
     *
     * Updates padding canvases using portions of video frame.
     *
     * @param stream - Captured video frame
     */
    protected updatePads(stream: HTMLCanvasElement): void;
    /**
     * Setup padding canvases
     *
     * Callback sets up size of padding canvases.
     */
    protected setupPadding: () => void;
}

/**
 * Generic video renderer
 *
 * VideoRenderer is based on {@link CanvasRenderer} and uses
 * two canvas layers: one for video stream and another to
 * render 3D scene on top of it. This usage pattern is the
 * easiest to implement, but more limited as video is not
 * embedded into the scene and e.g. renderer's postprocess
 * effects or advanced techniques can't be applied to video.
 * It's possible to draw on top of the video stream using
 * 2d canvas context, for example to add simple 2d effects.
 * Effects can be encapsulated in a form of render plugins.
 * Plugins are levels of abstraction allowing to single out
 * ready-made helpers that are used as atomic building blocks.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class VideoRenderer<ResultT extends {} = {}> extends CanvasRenderer<ResultT> {
    /** Drawing context of video canvas layer */
    videoCtx: CanvasRenderingContext2D | null;
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Update the video layer
     *
     * Draws input video frame on corresponding canvas layer.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Set video parameters
     *
     * Callback sets up size of video canvas layer.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}

/** Image source accepted by {@link ImageTexture} */
type ImageSource = ImageBitmap | ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement | Float32Array | Uint8Array;
/**
 * Image texture
 *
 * Helper class storing image as a WebGL texture.
 * Used by shader classes and renderers as wrapper
 * around images providing convenient interfaces.
 */
declare class ImageTexture {
    protected gl: WebGL2RenderingContext;
    protected size: Size;
    protected grayscale: boolean;
    protected linear: boolean;
    protected mipmap: boolean;
    /** Input image texture */
    protected buffer: WebGLTexture | null;
    /** Number of mipmap levels */
    protected levels: number;
    /**
     * Constructor
     *
     * @param gl - Context where texture is allocated
     * @param size - Size of the image
     * @param grayscale - Whether image is grayscsale or rgb
     * @param linear - Whether interpolation is linear or nearest
     */
    constructor(gl: WebGL2RenderingContext, size?: Size, grayscale?: boolean, linear?: boolean, mipmap?: boolean);
    /**
     * Update texture
     *
     * @param image - Image to upload into texture
     * @returns Updated texture
     */
    update(image: ImageSource): WebGLTexture | null;
    /**
     * Resize texture
     *
     * @param size - New size of the image
     * @returns True when successfully resized, false otherwise
     */
    resize(size: Size): boolean;
    /**
     * Dispose allocated resources
     */
    dispose(): void;
    /**
     * Texture instance
     *
     * @returns Instance of the texture
     */
    texture(): WebGLTexture | null;
    /**
     * Texture size
     *
     * @returns Size of the texture
     */
    textureSize(): Size;
    /**
     * Whether texture is allocated and valid
     *
     * @returns True if texture is allocated, false otherwise
     */
    valid(): boolean;
    /**
     * Whether texture is grayscale
     *
     * @returns True if texture is grayscale
     */
    isGrayscale(): boolean;
    /**
     * Generate mipmap levels if enabled
     */
    genLevels(): boolean;
    /**
     * Number of mipmap levels
     */
    levelCount(): number;
    /**
     * Size of the mipmap level
     */
    levelSize(level: number): Size;
}

/** Uniform types */
type UniformType = "1f" | "2f" | "3f" | "4f" | "1fv" | "2fv" | "3fv" | "4fv" | "1i" | "2i" | "3i" | "4i" | "1iv" | "2iv" | "3iv" | "4iv";
/**
 * Generic image processing shader program
 *
 * Helper class encapsulating all instances of a simple
 * image processing shaders, including vertex & fragment
 * shaders, output texture, frame buffer, and uniforms.
 * {@link ShaderProgram#process} method applies shaders
 * to input textures, results are written to the output
 * texture, {@link ShaderProgram#render} method renders
 * directly on the canvas element providing WebGL context.
 * Simplest program processes input texture covering it
 * it by 2 triangles and passing texture coordinates to
 * a fragment shader where all image processing happens.
 */
declare class ShaderProgram {
    protected gl: WebGL2RenderingContext;
    protected size: Size;
    protected inputs: string[];
    protected uniforms: {
        [key: string]: UniformType;
    };
    /** Vertex shader */
    protected vertShader: WebGLShader | null;
    /** Fragment shader */
    protected fragShader: WebGLShader | null;
    /** Shader program */
    protected shaderProgram: WebGLProgram | null;
    /** Vertex buffer */
    protected vertBuffer: WebGLBuffer | null;
    /** Vertex attributes */
    protected vertAttrs: WebGLVertexArrayObject | null;
    /** Unifrom locations */
    protected uniformsLoc: {
        [key: string]: WebGLUniformLocation | null;
    };
    /** Frame buffer (output) */
    protected frameBuffer: WebGLFramebuffer | null;
    /** Output image texture */
    protected outputTexture?: ImageTexture;
    /**
     * Constructor
     *
     * @param gl - WebGL context where program is instantiated
     * @param size - Size of processed (input & output) image
     * @param inputs - Shader texture inputs (names of sampler uniforms)
     * @param uniforms - Shader uniforms as name-type map object
     * @param fragSrc - Fragment shader source (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     * @param outGrayscale - Grayscale output texture
     * @param outLinear - Output texture read interpolation mode
     */
    constructor(gl: WebGL2RenderingContext, size?: Size, inputs?: string[], uniforms?: {
        [key: string]: UniformType;
    }, fragSrc?: string, vertSrc?: string, outGrayscale?: boolean, outLinear?: boolean, outMipmap?: boolean);
    /**
     * Process input image
     *
     * Applies shader program to input image, output
     * is written to image texture using framebuffer.
     *
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     * @returns Output image texture
     */
    process(inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): WebGLTexture | null;
    /**
     * Process input image and render the result
     *
     * Applies shader program to input image, output
     * is renderer to canvas providing WebGL context.
     *
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     */
    render(inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): void;
    /**
     * Output image texture
     *
     * @returns Image texture
     */
    output(): WebGLTexture | null;
    /**
     * Read image texture
     *
     * @returns Texture data
     */
    read(): Uint8Array | null;
    /**
     * Shader program instance
     *
     * @returns Instance of shader program
     */
    program(): WebGLProgram | null;
    /**
     * Resize
     *
     * Resizes output image texture and updates uniforms.
     *
     * @param size - Main/output shader size
     */
    resize(size: Size): void;
    /**
     * Set uniform value
     *
     * @param uniform - Uniform name
     * @param val - Uniform value
     */
    setUniform(uniform: string, val: number[]): void;
    /**
     * Dispose program object
     *
     * Releases resources and instances allocated by program.
     * Program object cannot be used after calling dispose().
     */
    dispose(): void;
    /**
     * Compile shader program
     *
     * Allocates and sets up all resources required
     * for shader program, compiles and links shaders.
     *
     * @param fragSrc - Fragment shader source (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     */
    protected compile(fragSrc: string, vertSrc: string): void;
    /**
     * Prepare execution of the shader
     *
     * Set input textures and provided uniform values,
     * bind program and setup vertex attribute arrays.
     *
     * @param gl - Context of shader program
     * @param inputs - Input image textures
     * @param uniforms - Values of shader uniforms
     */
    protected prepare(gl: WebGL2RenderingContext, inputs: (WebGLTexture | null)[], uniforms?: {
        [key: string]: number[];
    }): void;
    /**
     * Save context state
     *
     * Saves state of WebGL including program in use,
     * bound array buffers, framebuffer, and textures.
     * Returned state can be restored later to safely
     * share context with other frameworks or renderers.
     *
     * @param gl - Context which state will be recorded
     * @returns Current state of WebGL context
     */
    protected save(gl: WebGL2RenderingContext): {
        program: WebGLProgram | null;
        arrayBuffer: WebGLBuffer | null;
        framebuffer: WebGLFramebuffer | null;
        vertexArray: WebGLVertexArrayObject | null;
        viewport: Int32Array;
        scissor: Int32Array;
        colorMask: GLboolean[];
        cullFace: GLboolean;
        blend: GLboolean;
        activeTexture: GLenum;
        textures: (WebGLTexture | null)[];
    };
    /**
     * Restore context state
     *
     * Restores state of WebGL context to recorded checkpoint.
     * This allows to safely share context with other engines.
     *
     * @param gl - Context to restore state
     * @param state - Previously recorded state
     */
    protected restore(gl: WebGL2RenderingContext, state: ReturnType<ShaderProgram["save"]>): void;
    /**
     * Set uniform value unsafely
     *
     * @param uniform - Uniform name
     * @param val - Uniform value
     */
    protected setUniformUnsafe(uniform: string, val: number[]): void;
}

/**
 * Generic shader renderer
 *
 * ShaderRenderer is based on {@link CanvasRenderer} and uses
 * two canvas layers: one for video stream and another to
 * render 3D scene on top of it. Video is rendered by WebGL
 * shaders, this allows to apply complex computationally
 * demanding post-processing effects to the input stream.
 * For example, simple mono-chrome or sepia effects, or more
 * complex face beatification and dynamic geometry filters.
 * Shader effects can be encapsulated in a form of plugins.
 * Plugins are levels of abstraction allowing to single out
 * ready-made helpers that are used as atomic building blocks.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class ShaderRenderer<ResultT extends {} = {}> extends CanvasRenderer<ResultT> {
    /** Context of the video canvas layer */
    shaderCtx: WebGL2RenderingContext | null;
    /** Rendering shader */
    protected shader?: ShaderProgram;
    /** Input image texture */
    input?: ImageTexture;
    /** Current image texture */
    current: WebGLTexture | null;
    /**
     * Constructor
     *
     * @param params - Parameters of responsive canvas
     */
    constructor(params: CanvasParams);
    /**
     * Initialize renderer
     *
     * Initializes rendering context, shader program and buffers.
     *
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(): Promise<void>;
    /**
     * Reset renderer
     *
     * Releases all resources and instances created in load().
     * Releases rendering context, shader program and buffers.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the scene
     *
     * Renderers input video frame on corresponding canvas layer.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Update the video layer
     *
     * Calls rendering shaders with current input texture.
     *
     * @param stream - Captured video frame
     * @override
     */
    protected updateVideo(stream: HTMLCanvasElement): void;
    /**
     * Set video parameters
     *
     * Callback sets up size of video canvas layer,
     * resizes texture and allocates new storage.
     *
     * @param size - Resolution of input video
     * @param ratio - Aspect ration of input video
     * @override
     */
    setupVideo(size: Size, ratio?: number): void;
}

/**
 * Generic scene renderer
 *
 * Extends {@link VideoRenderer} to be used with the particular
 * WebGL engine e.g. Babylon.js or Three.js. Type of the scene
 * is additional parametrization of generic. {@link ScenePlugin}
 * written for WebGL engine can be attached to a SceneRenderer.
 * Usually they perform simple scene tasks that can be separated
 * from the main context into atomic building blocks, for example
 * control node of a scene to follow (be attached to) user's head
 * or replace its geometry with detected face mesh (mask effect).
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam SceneT - Type of renderer's scene
 */
declare class SceneRenderer<ResultT extends {} = {}, SceneT = undefined> extends ShaderRenderer<ResultT> {
    /** Renderer scene */
    scene?: SceneT;
    /**
     * Dispose renderer object
     *
     * Extended to dispose scene object.
     *
     * @override
     */
    dispose(): void;
}

/**
 * Generic video plugin
 *
 * VideoPlugin is a specialization of a {@link Plugin} for
 * {@link VideoRenderer}. Usually they perform simple 2D
 * drawing tasks on a video canvas (for example, simplest
 * face effects or adding debug information / graphics).
 * VideoPlugin gets access to 2d canvas of VideoRenderer
 * in load() and draws on this canvas directly in update(),
 * on top of video frame using provided processing data.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class VideoPlugin<ResultT extends {} = {}> extends Plugin<ResultT> {
    /** Drawing context of video canvas layer */
    protected videoCtx?: CanvasRenderingContext2D;
    /** Constructor */
    constructor();
    /**
     * Initialize plugin
     *
     * Initializes everything required for image drawing.
     * Acquires video canvas 2d context of the renderer.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources allocated in load().
     * Deletes the reference to 2d canvas context.
     *
     * @override
     */
    unload(): void;
}

/**
 * Generic shader plugin
 *
 * ShaderPlugin is a specialization of a {@link Plugin} for
 * {@link ShaderRenderer}. They apply complex computationally
 * demanding post-processing effects to the input stream.
 * For example, simple mono-chrome or sepia effects, or more
 * complex face beatification and dynamic geometry filters.
 * ShaderPlugin shares webgl context with the main renderer.
 * Basic implementation uses {@link ShaderProgram} created
 * for shaders provided to plugin's constructor. Plugins are
 * organized in chain within ShaderRenderer, input of the
 * next shader is output of the previous and initial input is
 * original video image, output of the last plugin is rendered.
 *
 * @typeParam ResultT - Type of processing results
 */
declare class ShaderPlugin<ResultT extends {} = {}> extends Plugin<ResultT> {
    protected inputs?: string[] | undefined;
    protected uniforms?: {
        [key: string]: UniformType;
    } | undefined;
    protected fragSrc?: string | undefined;
    protected vertSrc?: string | undefined;
    /** Rendering context */
    protected shaderCtx?: WebGL2RenderingContext;
    /** Rendering shader */
    protected shader?: ShaderProgram;
    /** Image size */
    protected size: Size;
    /**
     * Constructor
     *
     * @param inputs - Shader texture inputs (names of sampler uniforms)
     * @param uniforms - Shader uniforms as name-type map object
     * @param fragSrc - Code of fragment shader (copy shader by default)
     * @param vertSrc - Vertex shader source (copy shader by default)
     */
    constructor(inputs?: string[] | undefined, uniforms?: {
        [key: string]: UniformType;
    } | undefined, fragSrc?: string | undefined, vertSrc?: string | undefined);
    /**
     * Initialize plugin
     *
     * Initializes resources required for shader effect.
     * Acquires webgl context of the main webgl renderer.
     * Basic implementation creates and compiles rendering
     * program for shaders provided to plugin's constructor.
     * Overridden by derived classes for particular effect.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources and instances created in load().
     * Overridden by derived classes for particular effect.
     *
     * @override
     */
    unload(): void;
    /**
     * Update the image
     *
     * Main method implementing webgl shader effect or filter.
     * {@link @geenee/armature!ShaderRenderer | ShaderRenderer}
     * keeps track of the current iamge texture that will be
     * be rendered. ShaderPlugin uses the current texture as
     * input and writes results to {@link ShaderPlugin#output}.
     * {@link ShaderPlugin#output} becomes new current texture
     * of ShaderRenderer. This way all ShaderPlugins attached
     * to renderer organize a chain of effects applied on top
     * of each other. Method process() implements shader effect
     * itself, it's intended to be overridden by effect authors.
     *
     * @param result - Results of video processing
     * @param stream - Captured video frame
     * @returns Promise resolving when update is finished
     * @override @sealed
     */
    update(result: ResultT, stream: HTMLCanvasElement): Promise<void>;
    /**
     * Process the image
     *
     * Main method implementing webgl shader effect or filter.
     * It's called by update() and should be overridden to add
     * custom logic. process() should return true if effect is
     * successfully applied and false if it is skipped, for
     * example, if effect applies only when result is not empty.
     *
     * @param result - Results of video processing
     * @param input - Current image texture
     * @returns True on success, false otherwise
     * @virtual
     */
    process(result: ResultT, input: WebGLTexture): Promise<boolean>;
    /**
     * Set video size
     *
     * Adjusts shader and texture to a new size.
     *
     * @param size - Resolution of input video
     * @override
     */
    setupVideo(size: Size): void;
}

/**
 * Generic scene plugin
 *
 * ScenePlugins can be attached to SceneRenderer instances.
 * Usually they control a scene node and implement simple
 * tasks that can be separated from main rendering context.
 * For example, make a scene node follow (be attached to)
 * person's head, or make node an occluder, or create face
 * mesh node and set texture as a mask. On load() plugin
 * prepares or modifies the attached node if required and
 * reference to the scene object is cached to be used in
 * update() and unload(); update() implements main logic
 * and updates the scene node according to provided results.
 *
 * @typeParam ResultT - Type of processing results
 * @typeParam SceneT - Type of renderer's scene
 */
declare class ScenePlugin<ResultT extends {} = {}, SceneT = undefined> extends Plugin<ResultT> {
    /** Reference to a scene instance */
    protected scene?: SceneT;
    /** Constructor */
    constructor();
    /**
     * Initialize plugin
     *
     * Prepares or modifies the attached node if required.
     * Reference to the scene object is cached and used by
     * plugin on update() and unload(). You need to reload
     * plugin if you want to change scene it's attached to.
     *
     * @param renderer - Renderer this plugin is attached to
     * @returns Promise resolving when initialization is finished
     * @override
     */
    load(renderer: Renderer<ResultT>): Promise<void>;
    /**
     * Reset plugin
     *
     * Releases all resources allocated in load().
     * Deletes cached reference to the scene object.
     *
     * @override
     */
    unload(): void;
}

/**
 * Reader of image texture
 *
 * Helper class reading data from texture.
 * Used withing image processing pipeline
 * to get texture data on the client side.
 */
declare class TextureReader {
    protected gl: WebGL2RenderingContext;
    /** Frame buffer */
    protected frameBuffer: WebGLFramebuffer | null;
    protected pixelBuffer: WebGLBuffer | null;
    /**
     * Constructor
     *
     * @param gl - WebGL context where program is instantiated
     */
    constructor(gl: WebGL2RenderingContext);
    /**
     * Read texture data
     *
     * @param image - Image texture
     * @param level - Mipmap level
     * @returns Texture data
     */
    read(image: ImageTexture, level?: number): Uint8Array | null;
    /**
     * Non-blocking read of texture data
     *
     * @param image - Image texture
     * @param level - Mipmap level
     * @returns Texture data
     */
    readAsync(image: ImageTexture, level?: number): Promise<Uint8Array | null>;
    /**
     * Dispose texture reader
     *
     * Releases resources and instances allocated by reader.
     * Reader object cannot be used after calling dispose().
     */
    dispose(): void;
}

/**
 * Snapshot helper
 *
 * Takes a snapshot of the {@link ResponsiveCanvas} backing
 * a {@link CanvasRenderer}. In general, ResponsiveCanvas
 * is multi-layer therefore two capturing modes are available:
 * capture all layers separately or merge them into one image.
 * When you call snapshot() method Snapshoter waits for the
 * next render update and makes a copy of all canvas layers.
 * There're several modes for the resolution of the snapshot:
 * "video" - snapshot has the same size as the video stream,
 * "max"/"min" - in maximum/minimum size among canvas layers.
 */
declare class Snapshoter {
    protected renderer: CanvasRenderer;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    /**
     * Constructor
     *
     * @param renderer - Renderer to take snapshot of
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     */
    constructor(renderer: CanvasRenderer, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined);
    /**
     * Take snapshot of the renderer
     *
     * Enqueues capture after the next renderer update.
     * All canvas layers are merged into one final image.
     *
     * @returns Promise resolved to the merged captured image
     */
    snapshot(): Promise<ImageData | undefined>;
    /**
     * Take snapshot of the renderer
     *
     * Enqueues capture after the next renderer update.
     * All canvas layers are returned as separate images.
     *
     * @returns Promise resolved to the array of captured images
     */
    snapshotLayers(): Promise<(ImageData | undefined)[]>;
}

/**
 * Recorder helper
 *
 * Records a video of the {@link ResponsiveCanvas} backing a
 * {@link CanvasRenderer}. ResponsiveCanvas is multi-layer.
 * Every rendering update Recorder merges all snapshots onto
 * the recording canvas. Then canvas'es content is grabbed and
 * encoded by MediaRecorder. Encoded video chunks are cached to
 * later be merged into one blob containing the final video file.
 * There're several modes for resolution of the recorded video:
 * "video" - records in the same resolution as the video stream,
 * "max"/"min" - in maximum/minimum size among layers of canvas.
 * Additionally one can limit resolution and bitrate of a video.
 */
declare class Recorder {
    protected renderer: CanvasRenderer;
    protected type: string;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    protected bitRate?: number | undefined;
    /** Canvas to capture video */
    protected canvas: HTMLCanvasElement;
    /** Drawing context of capturing canvas */
    protected context: CanvasRenderingContext2D | null;
    /** Video stream */
    protected stream?: MediaStream;
    /** Video recorder */
    protected recorder?: MediaRecorder;
    /** Record chunks */
    protected records: Blob[];
    /**
     * Constructor
     *
     * @param renderer - Renderer to record video from
     * @param type - Media/video type
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     * @param bitRate - Video stream bit rate
     */
    constructor(renderer: CanvasRenderer, type?: string, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined, bitRate?: number | undefined);
    /**
     * Start video recording
     *
     * On every render update Recorder draws all layers onto
     * recording canvas. Chunks of encoded media stream are
     * cached to later be merged into one blob on stop().
     *
     * @returns True if recording started, False otherwise
     */
    start(): boolean;
    /**
     * Stop video recording
     *
     * Renderer stops recording and then merges encoded
     * video stream chunks into one blob returned to user.
     *
     * @returns Promise resolved to Blob containing encoded video
     */
    stop(): Promise<Blob | undefined>;
    /**
     * Dispose recorder object
     *
     * Releases resources and instances allocated by recorder.
     * Recorder object cannot be used after calling dispose().
     * One needs to stop recording before disposing the object.
     */
    dispose(): void;
    /**
     * Renderer update callback
     *
     * On every update Recorder draws all layers onto recording
     * canvas and requests video track to capture a new frame.
     */
    protected frame: () => void;
}

/**
 * Recorder helper (one layer verstion)
 *
 * Records a video of a layer of the {@link ResponsiveCanvas}
 * backing a {@link CanvasRenderer}. Simple and straightforward
 * recording of a canvas. Encoded video chunks are cached to
 * later be merged into one blob containing the final video file.
 */
declare class UniRecorder {
    protected renderer: CanvasRenderer;
    protected layer: number;
    protected type: string;
    protected bitRate?: number | undefined;
    /** Video stream */
    protected stream?: MediaStream;
    /** Video recorder */
    protected recorder?: MediaRecorder;
    /** Record chunks */
    protected records: Blob[];
    /**
     * Constructor
     *
     * @param renderer - Renderer to record video from
     * @param layer - Layer that will be recorded
     * @param type - Media/video type
     * @param bitRate - Video stream bit rate
     */
    constructor(renderer: CanvasRenderer, layer?: number, type?: string, bitRate?: number | undefined);
    /**
     * Start video recording
     *
     * Recorder encodes video stream associated with the
     * canvas layer. Chunks of encoded media stream are
     * cached to later be merged into one blob on stop().
     *
     * @returns True if recording started, False otherwise
     */
    start(): boolean;
    /**
     * Stop video recording
     *
     * Renderer stops recording and then merges encoded
     * video stream chunks into one blob returned to user.
     *
     * @returns Promise resolved to Blob containing encoded video
     */
    stop(): Promise<Blob | undefined>;
}

/**
 * Streamer helper
 *
 * Streams video of the {@link ResponsiveCanvas} backing a
 * {@link CanvasRenderer}. ResponsiveCanvas is multi-layer,
 * Every rendering update Streamer merges all snapshots onto
 * the recording canvas. MediaStream instance is created for
 * this canvas and provides access the generated video stream.
 * There're several modes for resolution of the streamed video:
 * "video" - streams in the same resolution as the video stream,
 * "max"/"min" - in maximum/minimum size among layers of canvas.
 * Additionally one can limit maximum resolution of the stream.
 */
declare class Streamer {
    protected renderer: CanvasRenderer;
    protected mirror: boolean;
    protected sizeMode: "video" | "max" | "min";
    protected sizeMax?: number | undefined;
    /** Canvas to capture video */
    protected canvas: HTMLCanvasElement;
    /** Drawing context of capturing canvas */
    protected context: CanvasRenderingContext2D | null;
    /** Output video stream */
    protected stream?: MediaStream;
    /**
     * Constructor
     *
     * @param renderer - Renderer to record video from
     * @param mirror - Mirror captured images
     * @param sizeMode - Video size mode
     * @param sizeMax - Maximum video size
     */
    constructor(renderer: CanvasRenderer, mirror?: boolean, sizeMode?: "video" | "max" | "min", sizeMax?: number | undefined);
    /**
     * Start video streaming
     *
     * On every Renderer update Streamer draws all layers onto
     * recording canvas. The canvas is a source of video stream.
     *
     * @returns True if recording started, False otherwise
     */
    start(): boolean;
    /**
     * Pause video streaming
     *
     * Stops drawing Renderer layers onto recording canvas.
     */
    pause(): void;
    /**
     * Media stream of generated video
     *
     * @returns Media stream
     */
    mediaStream(): MediaStream | undefined;
    /**
     * Render
     *
     * Every Renderer update Streamer merges snapshots of
     * multilayer ResponsiveCanvas onto recording canvas.
     */
    protected render: () => void;
}

export { type CanvasMode, type CanvasParams, CanvasRenderer, type ClipRect, Engine, type EngineParams, EventEmitterT, type FileParams, ImageBuffer, ImageCapture, type ImageInput, ImageTexture, Plugin, PluginOrdinal, PluginRenderer, type ProcParams, Processor, Recorder, Renderer, ResponsiveCanvas, ScenePlugin, SceneRenderer, ShaderPlugin, ShaderProgram, ShaderRenderer, type Size, Snapshoter, Streamer, TextureReader, UniRecorder, type UniformType, VideoCapture, type VideoParams, VideoPlugin, VideoRenderer, VideoSource, type VideoSourceParams };
